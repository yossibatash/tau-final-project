{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3e3d88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ea4a5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23128c49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre_trade_d - 18000.0\n",
      "trade_d - 21600.0\n",
      "post_trade_d - 12600\n"
     ]
    }
   ],
   "source": [
    "# Variables \n",
    "DATA_INTERVAL = 30 # 30 minute\n",
    "MIN = 60\n",
    "SEC = 60\n",
    "pre_trade_d = (5.5*(MIN*SEC)) - (DATA_INTERVAL*SEC)\n",
    "trade_d = 6.5*(MIN*SEC) - (DATA_INTERVAL*SEC)\n",
    "post_trade_d = 4*(MIN*SEC) - (DATA_INTERVAL*SEC)\n",
    "\n",
    "total_trade_d = pre_trade_d + trade_d + post_trade_d + (2*DATA_INTERVAL*SEC)\n",
    "\n",
    "pre_trade_ub = 5.5*(MIN*SEC)\n",
    "trade_ub = pre_trade_ub + 6.5*(MIN*SEC)\n",
    "post_trade_ub = trade_ub + 4*(MIN*SEC)\n",
    "\n",
    "print(f\"pre_trade_d - {pre_trade_d}\")\n",
    "print(f\"trade_d - {trade_d}\")\n",
    "print(f\"post_trade_d - {post_trade_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d96d20f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ybatash/.sdkman/candidates/spark/3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/15 17:49:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/04/15 17:49:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('pre_process_30min_data') \\\n",
    "          .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e653e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"time\", TimestampType()),\n",
    "    StructField(\"open\", DoubleType()),\n",
    "    StructField(\"high\", DoubleType()),\n",
    "    StructField(\"low\", DoubleType()),\n",
    "    StructField(\"close\", DoubleType()),\n",
    "    StructField(\"volume\", IntegerType())    \n",
    "])\n",
    "\n",
    "# Read 5 min csv files\n",
    "df = spark.read.option(\"header\",True).csv(\"./data/alpha_vantage/SPY/interval=30min/*\", schema=schema)\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2e38",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adding Time Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f38985",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- time: timestamp (nullable = true)\n",
      " |-- open: double (nullable = true)\n",
      " |-- high: double (nullable = true)\n",
      " |-- low: double (nullable = true)\n",
      " |-- close: double (nullable = true)\n",
      " |-- volume: integer (nullable = true)\n",
      " |-- t_trade_epoch: long (nullable = true)\n",
      " |-- t_trade_date: date (nullable = true)\n",
      " |-- t_trade_hour: integer (nullable = true)\n",
      " |-- t_trade_minute: integer (nullable = true)\n",
      " |-- t_trade_day_of_week: string (nullable = true)\n",
      " |-- t_trade_open_e: long (nullable = true)\n",
      " |-- t_trade_open_h: integer (nullable = true)\n",
      " |-- t_trade_close_e: long (nullable = true)\n",
      " |-- t_trade_close_h: integer (nullable = true)\n",
      " |-- t_time_diff_between_trade_and_open_in_sec: long (nullable = true)\n",
      " |-- t_trade_part: string (nullable = false)\n",
      " |-- t_trade_part_open_e: long (nullable = true)\n",
      " |-- t_trade_part_open_h: integer (nullable = true)\n",
      " |-- t_time_diff_between_trade_and_trade_part_open_in_sec: long (nullable = true)\n",
      " |-- t_precent_of_time_from_start_of_trade_pase: double (nullable = true)\n",
      " |-- t_precent_of_time_from_start_of_trade_day: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This column capture the time and date of SPY index price in epoch time.\n",
    "df = df.withColumn(\"t_trade_epoch\",F.unix_timestamp(\"time\"))\n",
    "\n",
    "# This column capture the date of SPY index price.\n",
    "df = df.withColumn(\"t_trade_date\", F.to_date(\"time\"))\n",
    "\n",
    "# This column capture the hour of SPY index price.\n",
    "df = df.withColumn(\"t_trade_hour\", F.hour(\"time\"))\n",
    "\n",
    "# This column capture the hour of SPY index price.\n",
    "df = df.withColumn(\"t_trade_minute\", F.minute(\"time\"))\n",
    "\n",
    "\n",
    "# This column capture the day of the week of SPY index price.\n",
    "df = df.withColumn(\"t_trade_day_of_week\", F.date_format(\"time\", 'E'))\n",
    "\n",
    "# e - epoch\n",
    "# This column capture the open epoch of SPY index price per day.\n",
    "df = df.withColumn(\"t_trade_open_e\", F.min(\"t_trade_epoch\").over(Window.partitionBy(\"t_trade_date\")))\n",
    "df = df.withColumn(\"t_trade_open_h\", F.hour(F.min(\"time\").over(Window.partitionBy(\"t_trade_date\"))))\n",
    "\n",
    "# This column capture the close epoch of SPY index price per day.\n",
    "df = df.withColumn(\"t_trade_close_e\", F.max(\"t_trade_epoch\").over(Window.partitionBy(\"t_trade_date\")))\n",
    "df = df.withColumn(\"t_trade_close_h\", F.hour(F.max(\"time\").over(Window.partitionBy(\"t_trade_date\"))))\n",
    "\n",
    "# This column capture the number of seconds pass from trade open and trade close.\n",
    "df = df.withColumn(\"t_time_diff_between_trade_and_open_in_sec\", F.col(\"t_trade_epoch\")-F.col(\"t_trade_open_e\"))\n",
    "\n",
    "# This column capture what part of the trade we are (pre trade, trade, post trade)\n",
    "df = df.withColumn(\"t_trade_part\", F.when(F.col(\"t_time_diff_between_trade_and_open_in_sec\") < pre_trade_ub,\"pre trade\")\n",
    ".when(F.col(\"t_time_diff_between_trade_and_open_in_sec\") < trade_ub,\"trade\")\n",
    ".when(F.col(\"t_time_diff_between_trade_and_open_in_sec\") <= post_trade_ub,\"post trade\")             \n",
    ".otherwise(\"Unknown\"))\n",
    "\n",
    "column_list = [\"t_trade_date\",\"t_trade_part\"]\n",
    "win_spec = Window.partitionBy([F.col(x) for x in column_list])\n",
    "\n",
    "# e - epoch\n",
    "# This column capture the open epoch of SPY index price per trading part in day.\n",
    "df = df.withColumn(\"t_trade_part_open_e\", F.min(\"t_trade_epoch\").over(win_spec))\n",
    "df = df.withColumn(\"t_trade_part_open_h\", F.hour(F.min(\"time\").over(win_spec)))\n",
    "\n",
    "# This column capture the number of seconds pass from trade part open and trade.\n",
    "df = df.withColumn(\"t_time_diff_between_trade_and_trade_part_open_in_sec\", F.col(\"t_trade_epoch\")-F.col(\"t_trade_part_open_e\"))\n",
    "\n",
    "df = df.withColumn(\"t_precent_of_time_from_start_of_trade_pase\", F.when(F.col(\"t_trade_part\") == \"pre trade\", F.col(\"t_time_diff_between_trade_and_trade_part_open_in_sec\")/pre_trade_d*100)\n",
    ".when(F.col(\"t_trade_part\") == \"trade\", F.col(\"t_time_diff_between_trade_and_trade_part_open_in_sec\")/trade_d*100)\n",
    ".when(F.col(\"t_trade_part\") == \"post trade\", F.col(\"t_time_diff_between_trade_and_trade_part_open_in_sec\")/post_trade_d*100)              \n",
    ".otherwise(-1))\n",
    "\n",
    "df = df.withColumn(\"t_precent_of_time_from_start_of_trade_day\", F.col(\"t_time_diff_between_trade_and_open_in_sec\")/total_trade_d*100)\n",
    "\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41036eda",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Adding Price Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding Open Price Features\n",
    "\n",
    "df = df.withColumn(\"rnk\",F.dense_rank().over(Window.partitionBy(\"t_trade_date\").orderBy(\"t_trade_epoch\")))\\\n",
    ".withColumn(\"p_min_open_price_for_last_90min\",F.min(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_avg_open_price_for_last_90min\",F.avg(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_max_open_price_for_last_90min\",F.max(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_stddev_open_price_for_last_90min\",F.stddev(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_min_open_price_for_last_150min\",F.min(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_avg_open_price_for_last_150min\",F.avg(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_max_open_price_for_last_150min\",F.max(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_stddev_open_price_for_last_150min\",F.stddev(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_min_open_price_for_last_300min\",F.min(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_avg_open_price_for_last_300min\",F.avg(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_max_open_price_for_last_300min\",F.max(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_stddev_open_price_for_last_300min\",F.stddev(\"open\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding high Price Features\n",
    "\n",
    "df = df.withColumn(\"rnk\",F.dense_rank().over(Window.partitionBy(\"t_trade_date\").orderBy(\"t_trade_epoch\")))\\\n",
    ".withColumn(\"p_min_high_price_for_last_90min\",F.min(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_avg_high_price_for_last_90min\",F.avg(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_max_high_price_for_last_90min\",F.max(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_stddev_high_price_for_last_90min\",F.stddev(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_min_high_price_for_last_150min\",F.min(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_avg_high_price_for_last_150min\",F.avg(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_max_high_price_for_last_150min\",F.max(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_stddev_high_price_for_last_150min\",F.stddev(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_min_high_price_for_last_300min\",F.min(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_avg_high_price_for_last_300min\",F.avg(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_max_high_price_for_last_300min\",F.max(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_stddev_high_price_for_last_300min\",F.stddev(\"high\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\n",
    "\n",
    "\n",
    "\n",
    "# df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Adding low Price Features\n",
    "\n",
    "df = df.withColumn(\"rnk\",F.dense_rank().over(Window.partitionBy(\"t_trade_date\").orderBy(\"t_trade_epoch\")))\\\n",
    ".withColumn(\"p_min_low_price_for_last_90min\",F.min(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_avg_low_price_for_last_90min\",F.avg(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_max_low_price_for_last_90min\",F.max(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_stddev_low_price_for_last_90min\",F.stddev(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_min_low_price_for_last_150min\",F.min(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_avg_low_price_for_last_150min\",F.avg(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_max_low_price_for_last_150min\",F.max(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_stddev_low_price_for_last_150min\",F.stddev(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_min_low_price_for_last_300min\",F.min(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_avg_low_price_for_last_300min\",F.avg(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_max_low_price_for_last_300min\",F.max(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_stddev_low_price_for_last_300min\",F.stddev(\"low\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\n",
    "\n",
    "\n",
    "# df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "# df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Adding close Price Features\n",
    "\n",
    "df = df.withColumn(\"rnk\",F.dense_rank().over(Window.partitionBy(\"t_trade_date\").orderBy(\"t_trade_epoch\")))\\\n",
    ".withColumn(\"p_min_close_price_for_last_90min\",F.min(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_avg_close_price_for_last_90min\",F.avg(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_max_close_price_for_last_90min\",F.max(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_stddev_close_price_for_last_90min\",F.stddev(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_min_close_price_for_last_150min\",F.min(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_avg_close_price_for_last_150min\",F.avg(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_max_close_price_for_last_150min\",F.max(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_stddev_close_price_for_last_150min\",F.stddev(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_min_close_price_for_last_300min\",F.min(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_avg_close_price_for_last_300min\",F.avg(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_max_close_price_for_last_300min\",F.max(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_stddev_close_price_for_last_300min\",F.stddev(\"close\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Adding volume Price Features\n",
    "\n",
    "df = df.withColumn(\"rnk\",F.dense_rank().over(Window.partitionBy(\"t_trade_date\").orderBy(\"t_trade_epoch\")))\\\n",
    ".withColumn(\"p_min_volume_price_for_last_90min\",F.min(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_avg_volume_price_for_last_90min\",F.avg(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_max_volume_price_for_last_90min\",F.max(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_stddev_volume_price_for_last_90min\",F.stddev(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-3,-1)))\\\n",
    ".withColumn(\"p_min_volume_price_for_last_150min\",F.min(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_avg_volume_price_for_last_150min\",F.avg(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_max_volume_price_for_last_150min\",F.max(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_stddev_volume_price_for_last_150min\",F.stddev(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-5,-1)))\\\n",
    ".withColumn(\"p_min_volume_price_for_last_300min\",F.min(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_avg_volume_price_for_last_300min\",F.avg(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_max_volume_price_for_last_300min\",F.max(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\\\n",
    ".withColumn(\"p_stddev_volume_price_for_last_300min\",F.stddev(\"volume\").over(Window.partitionBy(\"t_trade_date\").orderBy(\"rnk\").rangeBetween(-10,-1)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "\n",
    "df.printSchema()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "# select  --time, \n",
    "#         --t_trade_part,\n",
    "#         --t_precent_of_time_from_start_of_trade_pase,\n",
    "#         --t_precent_of_time_from_start_of_trade_day,     \n",
    "#         t_trade_epoch,\n",
    "#         rnk,\n",
    "#         open,\n",
    "#         p_min_open_price_for_last_90min,\n",
    "#         p_avg_open_price_for_last_90min,\n",
    "#         p_max_open_price_for_last_90min,\n",
    "#         p_stddev_open_price_for_last_90min,\n",
    "#         p_min_open_price_for_last_150min,\n",
    "#         p_avg_open_price_for_last_150min,\n",
    "#         p_max_open_price_for_last_150min,\n",
    "#         p_stddev_open_price_for_last_150min,\n",
    "#         p_min_open_price_for_last_300min,\n",
    "#         p_avg_open_price_for_last_300min,\n",
    "#         p_max_open_price_for_last_300min,\n",
    "#         p_stddev_open_price_for_last_300min\n",
    "       \n",
    "# from df\n",
    "# where t_trade_date = '2021-12-14'\n",
    "# order by time desc\n",
    "\n",
    "# \"\"\").show(10000,False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df93e547",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(\n",
    "# \"\"\"\n",
    "# select \n",
    "# time,\n",
    "# t_trade_date,\n",
    "# open,\n",
    "# p_avg_open_price_for_last_3min,\n",
    "# p_avg_open_price_for_last_5min,\n",
    "# p_avg_open_price_for_last_10min,\n",
    "# p_min_open_price_for_last_3min,\n",
    "# p_min_open_price_for_last_5min,\n",
    "# p_min_open_price_for_last_10min,\n",
    "# p_max_open_price_for_last_3min,\n",
    "# p_max_open_price_for_last_5min,\n",
    "# p_max_open_price_for_last_10min\n",
    "# from df\n",
    "# \"\"\"\n",
    "# ).show(100, False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.sql(\n",
    "# \"\"\"\n",
    "# select \n",
    "# t_trade_date,\n",
    "# time,\n",
    "# high,\n",
    "# p_min_high_price_for_last_3min,\n",
    "# p_min_high_price_for_last_5min,\n",
    "# p_min_high_price_for_last_10min,\n",
    "# p_max_high_price_for_last_3min,\n",
    "# p_max_high_price_for_last_5min,\n",
    "# p_max_high_price_for_last_10min,\n",
    "# p_avg_high_price_for_last_3min,\n",
    "# p_avg_high_price_for_last_5min,\n",
    "# p_avg_high_price_for_last_10min,\n",
    "# p_stddev_high_price_for_last_3min,\n",
    "# p_stddev_high_price_for_last_5min,\n",
    "# p_stddev_high_price_for_last_10min\n",
    "# from df\n",
    "# \"\"\"\n",
    "# ).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81751313",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# spark.sql(f\"\"\"\n",
    "# select time, \n",
    "#        t_trade_epoch,\n",
    "#        t_trade_open_e,\n",
    "#        t_trade_close_e,\n",
    "#        t_trade_part_open_e,\n",
    "\n",
    "#        t_trade_part,\n",
    "#        case\n",
    "#            when t_trade_part = 'pre trade' then {pre_trade_d}\n",
    "#            when t_trade_part = 'trade' then {trade_d}\n",
    "#            when t_trade_part = 'post trade' then {post_trade_d}\n",
    "#            else -1\n",
    "#        end as part_dur,\n",
    "           \n",
    "#        t_time_diff_between_trade_and_trade_part_open_in_sec,\n",
    "#        t_precent_of_time_from_start_of_trade_pase,\n",
    "#        t_precent_of_time_from_start_of_trade_day\n",
    "# from df\n",
    "# where t_trade_date = '2021-12-14'\n",
    "# order by time desc\n",
    "\n",
    "# \"\"\").show(10000,False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select *\n",
    "# from df2\n",
    "# where t_trade_open_h == 4\n",
    "# and t_trade_epoch = t_trade_open_e\n",
    "\n",
    "\n",
    "# \"\"\").show(100,False)\n",
    "\n",
    "\n",
    "# Test t_trade_open_e t_trade_close_e\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select count(*) rows_c, \n",
    "#        sum(case when t_trade_open_h == 4 then 1 else 0 end) open_h,\n",
    "#        sum(case when t_trade_close_h == 20 then 1 else 0 end) close_h\n",
    "# from df\n",
    "\n",
    "# \"\"\").show(100,False)\n",
    "\n",
    "# +------+------+-------+\n",
    "# |rows_c|open_h|close_h|\n",
    "# +------+------+-------+\n",
    "# |247708|247708|247212 |\n",
    "# +------+------+-------+\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select t_trade_open_h, t_trade_close_h, t_trade_date\n",
    "# from df\n",
    "# where t_trade_close_h <> 20\n",
    "# group by 1,2,3\n",
    "# \"\"\").show(100,False)\n",
    "\n",
    "# +--------------+---------------+------------+\n",
    "# |t_trade_open_h|t_trade_close_h|t_trade_date|\n",
    "# +--------------+---------------+------------+\n",
    "# |4             |17             |2022-11-25  |\n",
    "# +--------------+---------------+------------+\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "# select count(distinct t_trade_date) as date_c\n",
    "# from df\n",
    "# \"\"\").show(100,False)\n",
    "# +------+\n",
    "# |date_c|\n",
    "# +------+\n",
    "# |288   |\n",
    "# +------+\n",
    "\n",
    "\n",
    "# spark.sql(\"select time from df\").show(100,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a20e98b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test t_trade_part\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "\n",
    "# select t_trade_date,\n",
    "        \n",
    "#         --time, \n",
    "\n",
    "#        t_trade_hour,\n",
    "#        t_trade_part\n",
    "# from df\n",
    "# group by 1,2,3\n",
    "# \"\"\").show(10000,False)\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "\n",
    "# select count(*)\n",
    "# from df\n",
    "# where t_trade_part='Unknown'\n",
    "\n",
    "# \"\"\").show(100,False)\n",
    "\n",
    "# +--------+\n",
    "# |count(1)|\n",
    "# +--------+\n",
    "# |0       |\n",
    "# +--------+\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c09569",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Test t_trade_part_open_e and t_trade_part_open_h\n",
    "\n",
    "\n",
    "# df.select(F.col(\"time\"),\n",
    "#        F.col(\"t_trade_part\"),\n",
    "#        F.col(\"t_trade_part_open_e\"),\n",
    "#        F.col(\"t_precent_of_time_from_start_of_trade_day\"), \n",
    "#        F.col(\"t_time_diff_between_trade_and_trade_part_open_in_sec\"),           \n",
    "#        F.col(\"t_precent_of_time_from_start_of_trade_pase\")).show(10000,False)\n",
    "\n",
    "# df.filter(df[\"t_trade_date\"]=='2021-12-13')\\\n",
    "# .select(F.col(\"time\"),\n",
    "#        F.col(\"t_trade_epoch\"),\n",
    "#        F.col(\"t_trade_part_open_e\"),           \n",
    "#        F.col(\"t_time_diff_between_trade_and_open_in_sec\")).show(10000,False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# spark.sql(\"\"\"\n",
    "\n",
    "# select t_trade_date,\n",
    "        \n",
    "#        time, \n",
    "#        t_trade_part,\n",
    "#        t_trade_part_open_e\n",
    "# from df\n",
    "\n",
    "# \"\"\").show(10000,False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}