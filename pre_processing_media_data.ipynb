{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a3e3d88",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18ea4a5a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d96d20f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/ybatash/.sdkman/candidates/spark/3.2.1/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/04/22 18:22:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "          .appName('pre_process_media_data') \\\n",
    "          .getOrCreate()\n",
    "\n",
    "# spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e653e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- time_published: string (nullable = true)\n",
      " |-- ticker_sentiment: string (nullable = true)\n",
      " |-- filename: string (nullable = false)\n",
      " |-- file_ticker: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "schema = StructType([\n",
    "StructField(\"title\"                  , StringType()),   \n",
    "StructField(\"url\"                    , StringType()), \n",
    "StructField(\"time_published\"         , StringType()),             \n",
    "StructField(\"authors\"                , StringType()),     \n",
    "StructField(\"summary\"                , StringType()),     \n",
    "StructField(\"banner_image\"           , StringType()),           \n",
    "StructField(\"source\"                 , StringType()),     \n",
    "StructField(\"category_within_source\" , StringType()),                     \n",
    "StructField(\"source_domain\"          , StringType()),           \n",
    "StructField(\"topics\"                 , StringType()),     \n",
    "StructField(\"overall_sentiment_score\", StringType()),                     \n",
    "StructField(\"overall_sentiment_label\", StringType()),                     \n",
    "StructField(\"ticker_sentiment\"       , StringType())                   \n",
    "])\n",
    "\n",
    "# Read csv files\n",
    "df = spark.read \\\n",
    "        .option(\"header\",True) \\\n",
    "        .option(\"multiline\",True) \\\n",
    "        .option(\"quote\", \"\\\"\") \\\n",
    "        .option(\"escape\", \"\\\"\") \\\n",
    "        .csv(\"./data/alpha_vantage/news_data/*/*\", schema=schema)\n",
    "\n",
    "df = df.select(F.col(\"url\"),\n",
    "       F.col(\"time_published\"),\n",
    "       F.col(\"ticker_sentiment\"))\n",
    "\n",
    "df = df.withColumn(\"filename\", F.input_file_name())\n",
    "\n",
    "df = df.withColumn('file_ticker', F.split(df['filename'], '/').getItem(9))\n",
    "\n",
    "df.printSchema()\n",
    "\n",
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "357672eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many files we have per ticker\n",
    "files_per_ticker_df = spark.sql(\"\"\"\n",
    "select  ROW_NUMBER() OVER (ORDER BY count(*) desc) AS ROWNUM,\n",
    "        file_ticker, count(*)\n",
    "from df\n",
    "group by 2\n",
    "order by 3 desc\n",
    "\"\"\")\n",
    "\n",
    "# +------+-----------+--------+\n",
    "# |ROWNUM|file_ticker|count(1)|\n",
    "# +------+-----------+--------+\n",
    "# |1     |TSLA       |10227   |\n",
    "# |2     |JPM        |9954    |\n",
    "# |3     |AAPL       |9491    |\n",
    "# |4     |NFLX       |8994    |\n",
    "# |5     |BAC        |8634    |\n",
    "# |6     |MSFT       |8617    |\n",
    "# |7     |WFC        |7666    |\n",
    "# |8     |META       |7402    |\n",
    "# |9     |WMT        |7018    |\n",
    "# |10    |AMZN       |6792    |\n",
    "# |11    |PFE        |5708    |\n",
    "# |12    |NVDA       |5684    |\n",
    "# |13    |XOM        |3814    |\n",
    "# |14    |NKE        |3670    |\n",
    "# |15    |KO         |3376    |\n",
    "# |16    |JNJ        |3171    |\n",
    "# |17    |VZ         |2875    |\n",
    "# |18    |MA         |2611    |\n",
    "# |19    |COST       |2560    |\n",
    "# |20    |DIS        |2472    |\n",
    "# |21    |ABBV       |2430    |\n",
    "# |22    |CVX        |2378    |\n",
    "# |23    |HD         |2216    |\n",
    "# |24    |PEP        |2180    |\n",
    "# |25    |PG         |1812    |\n",
    "# |26    |MCD        |1757    |\n",
    "# |27    |CSCO       |1724    |\n",
    "# |28    |UNH        |1681    |\n",
    "# |29    |AVGO       |1649    |\n",
    "# |30    |ACN        |1557    |\n",
    "# |31    |V          |1375    |\n",
    "# |32    |ADBE       |963     |\n",
    "# |33    |ABT        |932     |\n",
    "# |34    |TMO        |760     |\n",
    "# |35    |PM         |722     |\n",
    "# |36    |LLY        |703     |\n",
    "# |37    |DHR        |443     |\n",
    "# |38    |TXN        |434     |\n",
    "# |39    |LIN        |213     |\n",
    "# +------+-----------+--------+\n",
    "\n",
    "files_per_ticker_df.createOrReplaceTempView('files_per_ticker_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55425808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- time_published: string (nullable = true)\n",
      " |-- ticker: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "schema = ArrayType(\n",
    "    StructType([StructField(\"ticker\", StringType()), \n",
    "                StructField(\"relevance_score\", StringType()), \n",
    "                StructField(\"ticker_sentiment_score\", StringType()), \n",
    "                StructField(\"ticker_sentiment_label\", StringType())]))\n",
    "\n",
    "\n",
    "df = df.withColumn(\"ticker_sentiment_new\", from_json(F.col(\"ticker_sentiment\"), schema))\n",
    "\n",
    "df = df.withColumn(\"ticker_sentiment_item\",F.explode(F.col(\"ticker_sentiment_new\")))\n",
    "\n",
    "df = df.withColumn(\"ticker\", F.col(\"ticker_sentiment_item\").getItem(\"ticker\"))\n",
    "\n",
    "df = df.withColumn(\"relevance_score\", F.col(\"ticker_sentiment_item\").getItem(\"relevance_score\"))\n",
    "\n",
    "df = df.withColumn(\"ticker_sentiment_score\", F.col(\"ticker_sentiment_item\").getItem(\"ticker_sentiment_score\"))\n",
    "\n",
    "df = df.withColumn(\"ticker_sentiment_label\", F.col(\"ticker_sentiment_item\").getItem(\"ticker_sentiment_label\"))\n",
    "\n",
    "df = df.select(F.col(\"url\"),\n",
    "               F.col(\"time_published\"),\n",
    "               F.col(\"ticker\"))\n",
    "\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView('df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98d076d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- time_published: string (nullable = true)\n",
      " |-- ticker_count_per_article: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ticker_count_per_article_df = spark.sql(\"\"\"\n",
    "with top_tickers\n",
    "as \n",
    "(\n",
    "    select * \n",
    "    from files_per_ticker_df\n",
    "    where ROWNUM<36\n",
    "),\n",
    "s1\n",
    "as\n",
    "(\n",
    "    select url,\n",
    "           ticker,\n",
    "           min(time_published) as time_published     \n",
    "    from   df\n",
    "    group by 1,2\n",
    ")\n",
    "\n",
    "select s1.url,\n",
    "       s1.time_published,\n",
    "       count(s1.ticker) as ticker_count_per_article\n",
    "from   s1 join top_tickers\n",
    "       on s1.ticker = top_tickers.file_ticker \n",
    "group by 1,2\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "ticker_count_per_article_df.createOrReplaceTempView(\"ticker_count_per_article_df\")\n",
    "\n",
    "ticker_count_per_article_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39d358a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- t_article_published_date: string (nullable = false)\n",
      " |-- t_article_published_ts: timestamp (nullable = true)\n",
      " |-- t_article_published_epoch: long (nullable = true)\n",
      " |-- ticker_count_per_article: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This column capture the year of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_year\", \n",
    "                                                                     F.col(\"time_published\")[0:4])\n",
    "\n",
    "# This column capture the month of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_month\", \n",
    "                                                                     F.col(\"time_published\")[5:2])\n",
    "\n",
    "# This column capture the day (in the month) of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_day\", \n",
    "                                                                     F.col(\"time_published\")[7:2])\n",
    "\n",
    "# This column capture the hour of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_hour\", \n",
    "                                                                     F.col(\"time_published\")[10:2])\n",
    "\n",
    "# This column capture the minute of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_min\", \n",
    "                                                                     F.col(\"time_published\")[12:2])\n",
    "\n",
    "# This column capture the second of the media article been published.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_sec\",\n",
    "                                                                     F.col(\"time_published\")[14:2])\n",
    "\n",
    "# This column capture the date of the media article.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_date\", \n",
    "                                                                     F.concat_ws('-',\n",
    "                                                                                  F.col(\"t_article_published_year\"),\n",
    "                                                                                  F.col(\"t_article_published_month\"),\n",
    "                                                                                  F.col(\"t_article_published_day\")\n",
    "                                                                                )\n",
    "                                                                    )\n",
    "\n",
    "# This column capture the time of the media article.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_time\", \n",
    "                                                                     F.concat_ws(':',\n",
    "                                                                                  F.col(\"t_article_published_hour\"),\n",
    "                                                                                  F.col(\"t_article_published_min\"),\n",
    "                                                                                  F.col(\"t_article_published_sec\")\n",
    "                                                                                )\n",
    "                                                                    )\n",
    "\n",
    "# This column capture the timestamp of the media article.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_ts\", \n",
    "                                                                     F.to_timestamp(\n",
    "                                                                         F.concat_ws(' ',\n",
    "                                                                                     F.col(\"t_article_published_date\"),\n",
    "                                                                                     F.col(\"t_article_published_time\")\n",
    "                                                                                    )\n",
    "                                                                                   )\n",
    "                                                                    )\n",
    "\n",
    "# This column capture the time and date of the media article in epoch time.\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.withColumn(\"t_article_published_epoch\", \n",
    "                                                                     F.unix_timestamp(\"t_article_published_ts\"))\n",
    "\n",
    "\n",
    "ticker_count_per_article_df = ticker_count_per_article_df.select(F.col(\"url\"),\n",
    "                                                                 F.col(\"t_article_published_date\"),        \n",
    "                                                                 F.col(\"t_article_published_ts\"),\n",
    "                                                                 F.col(\"t_article_published_epoch\"),                                                                 \n",
    "                                                                 F.col(\"ticker_count_per_article\")\n",
    "                                                                )\n",
    "\n",
    "ticker_count_per_article_df.createOrReplaceTempView(\"ticker_count_per_article_df\")\n",
    "\n",
    "ticker_count_per_article_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0525c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding for each time resolution (1,5,10,15,30 minute) the proper key (round up epoch key)\n",
    "\n",
    "before_agg_df = spark.sql(\"\"\"\n",
    "\n",
    "with round_epoch\n",
    "as\n",
    "(\n",
    "select url,\n",
    "       t_article_published_ts,\n",
    "       t_article_published_epoch,\n",
    "       cast((t_article_published_epoch/60) as int)*60                 as  t_nearest_min_epoch,\n",
    "       cast((t_article_published_epoch/(60*5)) as int)*(60*5)         as  t_nearest_5min_epoch,\n",
    "       cast((t_article_published_epoch/(60*10)) as int)*(60*10)       as  t_nearest_10min_epoch,\n",
    "       cast((t_article_published_epoch/(60*15)) as int)*(60*15)       as  t_nearest_15min_epoch,\n",
    "       cast((t_article_published_epoch/(60*30)) as int)*(60*30)       as  t_nearest_30min_epoch,\n",
    "       ticker_count_per_article          \n",
    "from ticker_count_per_article_df\n",
    "\n",
    ")\n",
    "select url                                                as url,\n",
    "       t_nearest_min_epoch                                as t_round_up_min_epoch,\n",
    "       case \n",
    "            when t_nearest_5min_epoch<t_nearest_min_epoch \n",
    "                then t_nearest_5min_epoch+(60*5)\n",
    "            else t_nearest_5min_epoch\n",
    "       end                                                as t_round_up_5min_epoch,\n",
    "       case \n",
    "            when t_nearest_10min_epoch<t_nearest_min_epoch \n",
    "                then t_nearest_10min_epoch+(60*10)\n",
    "            else t_nearest_10min_epoch\n",
    "       end                                                as t_round_up_10min_epoch,\n",
    "       case \n",
    "            when t_nearest_15min_epoch<t_nearest_min_epoch \n",
    "                then t_nearest_15min_epoch+(60*15)\n",
    "            else t_nearest_15min_epoch\n",
    "       end                                                as t_round_up_15min_epoch,\n",
    "       case \n",
    "            when t_nearest_30min_epoch<t_nearest_min_epoch \n",
    "                then t_nearest_30min_epoch+(60*30)\n",
    "            else t_nearest_30min_epoch\n",
    "       end                                                as t_round_up_30min_epoch,\n",
    "       ticker_count_per_article                           as ticker_count_per_article\n",
    "from round_epoch\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "before_agg_df.createOrReplaceTempView(\"before_agg_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4b4a627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- url: string (nullable = true)\n",
      " |-- t_round_up_min_epoch: integer (nullable = true)\n",
      " |-- t_round_up_5min_epoch: integer (nullable = true)\n",
      " |-- t_round_up_10min_epoch: integer (nullable = true)\n",
      " |-- t_round_up_15min_epoch: integer (nullable = true)\n",
      " |-- t_round_up_30min_epoch: integer (nullable = true)\n",
      " |-- ticker_count_per_article: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "before_agg_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4e44575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- epoch_key: integer (nullable = true)\n",
      " |-- snp_ticker_count_epoch: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:22:28 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:22:44 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:22:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:22:51 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------------+\n",
      "|summary|          epoch_key|snp_ticker_count_epoch|\n",
      "+-------+-------------------+----------------------+\n",
      "|  count|              75243|                 75243|\n",
      "|   mean|1.661516264525338E9|     2.073774304586473|\n",
      "| stddev|  8742703.295124054|    2.1689905026486476|\n",
      "|    min|         1646114400|                     1|\n",
      "|    25%|         1653905700|                     1|\n",
      "|    50%|         1661398320|                     1|\n",
      "|    75%|         1668690960|                     2|\n",
      "|    max|         1677437460|                    86|\n",
      "+-------+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregate data for 1 min resolution data sets.\n",
    "\n",
    "agg_1min_data = spark.sql(\"\"\"\n",
    "\n",
    "select t_round_up_min_epoch          as epoch_key, \n",
    "       sum(ticker_count_per_article) as snp_ticker_count_epoch\n",
    "from before_agg_df\n",
    "group by 1\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "agg_1min_data.printSchema()\n",
    "\n",
    "agg_1min_data.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "117ca23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- epoch_key: integer (nullable = true)\n",
      " |-- snp_ticker_count_epoch: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:22:56 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:05 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:06 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 43:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------------+\n",
      "|summary|          epoch_key|snp_ticker_count_epoch|\n",
      "+-------+-------------------+----------------------+\n",
      "|  count|              48436|                 48436|\n",
      "|   mean|1.661711178003964E9|    3.2215087951110744|\n",
      "| stddev|   8807269.59170083|    3.2075399973287673|\n",
      "|    min|         1646114400|                     1|\n",
      "|    25%|         1654087200|                     1|\n",
      "|    50%|         1661675400|                     2|\n",
      "|    75%|         1669105500|                     4|\n",
      "|    max|         1677437700|                    87|\n",
      "+-------+-------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Agregate data for 5 min resolution data sets.\n",
    "\n",
    "agg_5min_data = spark.sql(\"\"\"\n",
    "\n",
    "select t_round_up_5min_epoch          as epoch_key, \n",
    "       sum(ticker_count_per_article) as snp_ticker_count_epoch\n",
    "from before_agg_df\n",
    "group by 1\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "agg_5min_data.printSchema()\n",
    "\n",
    "agg_5min_data.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6b51133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- epoch_key: integer (nullable = true)\n",
      " |-- snp_ticker_count_epoch: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:23:09 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:19 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 70:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------------+\n",
      "|summary|           epoch_key|snp_ticker_count_epoch|\n",
      "+-------+--------------------+----------------------+\n",
      "|  count|               33038|                 33038|\n",
      "|   mean|1.6618145357103941E9|     4.722955384708517|\n",
      "| stddev|   8852893.345211979|    4.5157458798135615|\n",
      "|    min|          1646114400|                     1|\n",
      "|    25%|          1654152600|                     2|\n",
      "|    50%|          1661841600|                     3|\n",
      "|    75%|          1669256400|                     6|\n",
      "|    max|          1677438000|                    87|\n",
      "+-------+--------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Agregate data for 10 min resolution data sets.\n",
    "\n",
    "agg_10min_data = spark.sql(\"\"\"\n",
    "\n",
    "select t_round_up_10min_epoch          as epoch_key, \n",
    "       sum(ticker_count_per_article) as snp_ticker_count_epoch\n",
    "from before_agg_df\n",
    "group by 1\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "agg_10min_data.printSchema()\n",
    "\n",
    "agg_10min_data.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20845b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- epoch_key: integer (nullable = true)\n",
      " |-- snp_ticker_count_epoch: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:23:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:37 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:38 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 97:>                                                       (0 + 10) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------------+\n",
      "|summary|           epoch_key|snp_ticker_count_epoch|\n",
      "+-------+--------------------+----------------------+\n",
      "|  count|               25093|                 25093|\n",
      "|   mean|1.6618471986769218E9|     6.218347746383453|\n",
      "| stddev|   8881403.978248285|    5.8359004827999685|\n",
      "|    min|          1646114400|                     1|\n",
      "|    25%|          1654159500|                     2|\n",
      "|    50%|          1661882400|                     4|\n",
      "|    75%|          1669359600|                     9|\n",
      "|    max|          1677438000|                    98|\n",
      "+-------+--------------------+----------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Agregate data for 15 min resolution data sets.\n",
    "\n",
    "agg_15min_data = spark.sql(\"\"\"\n",
    "\n",
    "select t_round_up_15min_epoch          as epoch_key, \n",
    "       sum(ticker_count_per_article) as snp_ticker_count_epoch\n",
    "from before_agg_df\n",
    "group by 1\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "agg_15min_data.printSchema()\n",
    "\n",
    "agg_15min_data.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3685b9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- epoch_key: integer (nullable = true)\n",
      " |-- snp_ticker_count_epoch: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:23:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:48 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:23:54 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------------------+\n",
      "|summary|           epoch_key|snp_ticker_count_epoch|\n",
      "+-------+--------------------+----------------------+\n",
      "|  count|               14729|                 14729|\n",
      "|   mean|1.6618985156765563E9|     10.59386244823138|\n",
      "| stddev|   8926061.065160422|      9.70017137121795|\n",
      "|    min|          1646114400|                     1|\n",
      "|    25%|          1654149600|                     3|\n",
      "|    50%|          1661970600|                     7|\n",
      "|    75%|          1669554000|                    16|\n",
      "|    max|          1677438000|                   109|\n",
      "+-------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregate data for 30 min resolution data sets.\n",
    "\n",
    "agg_30min_data = spark.sql(\"\"\"\n",
    "\n",
    "select t_round_up_30min_epoch          as epoch_key, \n",
    "       sum(ticker_count_per_article) as snp_ticker_count_epoch\n",
    "from before_agg_df\n",
    "group by 1\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "agg_30min_data.printSchema()\n",
    "\n",
    "agg_30min_data.summary().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a85df687",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/04/22 18:25:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:12 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:17 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:39 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:40 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:25:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:01 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:02 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:04 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:14 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:20 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:21 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:34 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "23/04/22 18:26:36 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# write data set into DWH\n",
    "agg_1min_data.write.parquet(\"/Users/ybatash/Workspace/tau.ac.il/tau-final-project/data/DWH/media_aggregated_data/interval=1min/\")\n",
    "\n",
    "agg_5min_data.write.parquet(\"/Users/ybatash/Workspace/tau.ac.il/tau-final-project/data/DWH/media_aggregated_data/interval=5min/\")\n",
    "\n",
    "agg_10min_data.write.parquet(\"/Users/ybatash/Workspace/tau.ac.il/tau-final-project/data/DWH/media_aggregated_data/interval=10min/\")\n",
    "\n",
    "agg_15min_data.write.parquet(\"/Users/ybatash/Workspace/tau.ac.il/tau-final-project/data/DWH/media_aggregated_data/interval=15min/\")\n",
    "\n",
    "agg_30min_data.write.parquet(\"/Users/ybatash/Workspace/tau.ac.il/tau-final-project/data/DWH/media_aggregated_data/interval=30min/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbb2669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d21e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
